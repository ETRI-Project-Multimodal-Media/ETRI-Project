import os
import json
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from tqdm import tqdm

# ğŸ“‚ JSON íŒŒì¼ì´ ë“¤ì–´ ìˆëŠ” í´ë”
input_folder = "/home/kylee/LongVALE/logs"
output_file = "/home/kylee/LongVALE/logs/postprocessing.jsonl"

# âœ… ì‚¬ìš©í•  LLM ëª¨ë¸ (ë¡œê·¸ì¸ í•„ìš” ì—†ëŠ” ê³µê°œ ëª¨ë¸ ì¶”ì²œ)
model_id = "meta-llama/Meta-Llama-3-8B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

def split_caption_with_llm(caption_text: str) -> str:
    """LLMì„ ì´ìš©í•´ Visual/Audio/Speechë¡œ ë¶„ë¦¬"""
    # Few-shot í”„ë¡¬í”„íŠ¸
    prompt = """
    You are a helpful assistant that splits a multimodal caption into three parts: 
    Visual, Audio, and Speech.

    Examples:
    Input: "A woman is playing the piano while singing 'I love you.' Applause can be heard."
    Output:
    Visual: "A woman is playing the piano."
    Audio: "Applause can be heard."
    Speech: "'I love you,' she sings."

    Input: "A man is playing the guitar while singing 'I love you.' The acoustic guitar sound resonates in the background."
    Output:
    Visual: "A man is playing the guitar."
    Audio: "The acoustic guitar sound resonates in the background."
    Speech: "'I love you,' he sings."

    ---

    Now split the following:

    Input: "{}"
    Output:
    """.format(caption_text)

    messages = [
        {"role": "system", "content": "You are an assistant that classifies captions into Visual, Audio, and Speech."},
        {"role": "user", "content": prompt},
    ]

    input_ids = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(model.device)

    terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>")
    ]

    outputs = model.generate(
        input_ids,
        max_new_tokens=2048,
        eos_token_id=terminators,
        do_sample=True,
        temperature=0.6,
        top_p=0.9,
    )

    response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)
    return response.strip()


from tqdm import tqdm

def extract_answer_from_line(line: str) -> str:
    """
    í•œ ì¤„ ë¬¸ìì—´ì—ì„œ "answer": "..." ë¶€ë¶„ë§Œ ë½‘ì•„ëƒ„
    """
    key = '"answer":'
    start = line.find(key)
    if start == -1:
        return ""

    # answer ë’¤ ì²« ë”°ì˜´í‘œ
    start = line.find('"', start + len(key))
    if start == -1:
        return ""

    # answer ë ë”°ì˜´í‘œ
    end = line.find('"', start + 1)
    if end == -1:
        return ""

    return line[start + 1:end]


def process_txt_file(input_file, output_file):
    results = []

    with open(input_file, "r", encoding="utf-8") as f:
        lines = f.readlines()

    for idx, line in enumerate(tqdm(lines, desc="Processing lines")):
        line = line.strip()
        if not line:
            continue

        # âœ… answerë§Œ ì¶”ì¶œ
        caption_text = extract_answer_from_line(line)
        if not caption_text:
            continue

        # ğŸ‘‰ LLM ì‹¤í–‰
        split_result = split_caption_with_llm(caption_text)

        # ê²°ê³¼ ì €ì¥
        result_entry = {
            "line_id": idx,
            "original_answer": caption_text,
            "split_caption": split_result
        }
        results.append(result_entry)

        with open(output_file, "a", encoding="utf-8") as out_f:
            out_f.write(str(result_entry) + "\n")

    return results


if __name__ == "__main__":
    input_file = "/home/kylee/LongVALE/logs/eval.txt"      # ì²˜ë¦¬í•  TXT íŒŒì¼
    output_file = "/home/kylee/LongVALE/logs/LLAMA_postprocess.txt"   # ê²°ê³¼ ì €ì¥ íŒŒì¼
    process_txt_file(input_file, output_file)
    print(f"âœ… ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")