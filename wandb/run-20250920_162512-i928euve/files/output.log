Loading LongVALE-LLM from base model...
Traceback (most recent call last):
  File "/home/kylee/LongVALE/longvalellm/eval/eval.py", line 90, in <module>
    tokenizer, model, context_len = load_pretrained_model(args, args.stage2, args.stage3)
  File "/home/kylee/LongVALE/longvalellm/eval/../../longvalellm/model/builder.py", line 31, in load_pretrained_model
    tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)
  File "/home/kylee/anaconda3/envs/longvale/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 1125, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/home/kylee/anaconda3/envs/longvale/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2142, in __getattribute__
    requires_backends(cls, cls._backends)
  File "/home/kylee/anaconda3/envs/longvale/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2128, in requires_backends
    raise ImportError("".join(failed))
ImportError:
LlamaTokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the
installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.
